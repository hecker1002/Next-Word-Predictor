{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Text = ''' Our ‘Leading voices’ article series features school principals from across India. With 28 years’ experience, Dr Priyanka Mehta has won numerous education awards for her achievements. She is currently Principal Director at Sarvottam International School, Greater Noida. Here, Anannya Chakraborty asks Dr Mehta 5 questions, including what school education could look like in the future, and the advice she’d give to teachers who are looking to move into a leadership role.\n",
    "\n",
    "What advice would you give to an educator looking to move into a leadership role?\n",
    "\n",
    "For an educator who plans to be in a leadership role, it is important to remember that they can never cease to be a teacher and, more importantly, a learner. Leadership requires continuous and committed learning and upgrading of skills.\n",
    "\n",
    "One needs to lead by example. Being honest and exhibiting the values of the head and heart is the mantra for mentoring strong teams and long-term relationships. Collaboration and networking provide insights and exposure.\n",
    "\n",
    "As an educator, we can become secretive, trying to do things all by ourselves. As a leader, we must learn to delegate and empower. Trusting people may be a difficult habit to form, but it is absolutely necessary.\n",
    "\n",
    "Also, shifting roles [from teacher to leader] will require a lot of effort, especially in the case of embracing change and being innovative. Designing solutions is the key to good decisions, which is the core of leadership.\n",
    "\n",
    "Finally, looking within is very important. Self-awareness is a great strength. One must never forget that leadership is a journey and takes time. Each day is an experience to learn from.\n",
    "\n",
    "\n",
    "Who has been the most important leadership mentor in your career and why is it important for principals to have a mentor they can call on for support?\n",
    "\n",
    "In all these years of service, I have had the privilege of learning from innumerable mentors. It will not be fair to mention just one. Even children have been a huge source of inspiration to me.\n",
    "\n",
    "As a school head, I fondly read and get motivated by leaders in education, politics, and research; the list is never-ending!\n",
    "\n",
    "On a personal front, my father is my inspiration. He has taught me to be upright and compassionate and never to compromise on values. One of his fondest teachings is – karma comes back to you.\n",
    "\n",
    "Leave alone principals, every single person needs a mentor for support. This is not a sign of inadequacy or weakness. Accuracy in decision making is vital in our case, as we deal with children and diverse people. Seeking good counsel from an experienced senior person is a safety net. You really need someone to talk to you besides the voices in your head.\n",
    "\n",
    "What do you know now that you wish you’d known at the start of your career?\n",
    "\n",
    "As a young teacher, I thought teaching was all about knowing the content and planning to deliver it. The years have taught me that it is all about listening.\n",
    "\n",
    "Believe in children more than you believe in your skills. Teaching gets easier with time, so there is no need to rush.\n",
    "\n",
    "With almost 20 years as a head of the school now, I wish I knew that being at the top gets lonely at times. It is overwhelming sometimes to realise that the buck stops with you − always! That relationships will change, and it is impossible to control everything.\n",
    "\n",
    "I am a people person. So, it took me some time to adjust to the fact that not everyone will like what I do or do things in the same way that I want. The fact of the day is that differences make all the difference.\n",
    "\n",
    "A very important lesson has been that one must learn from others without losing too much of themselves.\n",
    "\n",
    "What do you think school education will look like in 2050?\n",
    "\n",
    "Honestly, that’s a tough one to answer. The pace of change around you makes you nervous sometimes. Whatever it may be like, I hope we are able to keep the human connection in education alive. A teacher will not vanish from the scene, but her role will see a lot of transformation which, at the moment, is challenging to predict.\n",
    "\n",
    "However, to answer for once, I would say that a lot would depend on what technology would be like, the changes in society, and the state of human development.\n",
    "\n",
    "I am truly looking forward to personalised learning, helping children work on individual gaps through adaptive learning platforms. We would have done away with mundane content and routines of learning. Research, design, and innovation are the future.\n",
    "\n",
    "We have already incorporated blended learning, skill- and competency-based modules and global perspectives. This will surely fade away boundaries and social taboos. Virtual learning is sure to find a place in the future.\n",
    "\n",
    "I also feel that the study of sustainability and pure sciences through an interdisciplinary approach may gain popularity.\n",
    "\n",
    "What do you consider to be the biggest challenge facing school leaders in India?\n",
    "\n",
    "Handling diversity in the classrooms and delivering with equity is the biggest challenge we face. Currently we are in a transition phase. Fortunately, we are all convinced for change in curriculum and assessment and alignment with futuristic models, yet, to deliver the same PAN India in the way planned, is a challenge due to heterogeneity.\n",
    "\n",
    "Further, appointing the right kind of teacher training faculty has always been a battle.\n",
    "\n",
    "As a school leader, how often do you set aside time to speak to mentors and members of your own support network? How does this help your own practice and reflection? How can you strengthen these networks?'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\nTAke a sentence word-by-word and break it into - INput and Output at each step and train thre LSTM \\non itfor next-word-prediction (supervised learning ) .\\n\\ne.g. - I am Aryan \\n\\nIP I , OP am .\\nIP I am , OP Aryan .\\n\\ntrain LSTM on (IP , OP ) and do LSTM.precit(IP1)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "\n",
    "TAke a sentence word-by-word and break it into - INput and Output at each step and train thre LSTM \n",
    "on itfor next-word-prediction (supervised learning ) .\n",
    "\n",
    "e.g. - I am Aryan \n",
    "\n",
    "IP I , OP am .\n",
    "IP I am , OP Aryan .\n",
    "\n",
    "train LSTM on (IP , OP ) and do LSTM.precit(IP1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer  # to tokenize the sentence intow words which helps to  preprocess them and makes vocab of unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = Tokenizer()\n",
    "\n",
    "tokenize.fit_on_texts( [Text] )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize.word_index -> to see index of each word in vocbbulary of words \n",
    "\n",
    "# list of input_sequences (vector of numbers )\n",
    "\n",
    "input_sequences = []\n",
    "output_sequences  = []\n",
    "\n",
    "\n",
    "#a list of sentences form our text \n",
    "# Text.split('.' or ',') \n",
    "\n",
    "for sentence in Text.split('.' or ',') :\n",
    "    tokenized_sentences = tokenize.texts_to_sequences( [sentence] )[0]  # coverted words to a vector of numbers ( using vocabulary of unique words )\n",
    "    # extra [0] to remove lists of list , just make it a simple lsit \n",
    "\n",
    "    # now , make a dataset of IP and Op (from tokenzed sentences as vector of numbers ) by selecting word one-by-one\n",
    "    # Ip- bag of N-words (N-Gram )\n",
    "\n",
    "    for i in range(1 , len(tokenized_sentences)) :\n",
    "        ip = tokenized_sentences[: i +1  ] # N-gram (bag of n words input for LSTM )\n",
    "        \n",
    "\n",
    "        #adding the vectorized words of each sentence in IP/OP sequrntially \n",
    "        input_sequences.append(ip)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[69, 125]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_seq - N-gram ( sequntiual sentenc e)\\\n",
    "# output sequences  = next word index \n",
    "\n",
    "# to amke each tokenized sentence ( [2 1 ] ) of equal length ( since passing it to LSTM )\n",
    " # use padding \n",
    "\n",
    "max_len = max([ len(x) for x in input_sequences]) # finding longest possible length of word in given sequence \n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "\n",
    "pad_inp_seq = pad_sequences(input_sequences , maxlen = max_len , padding = 'pre')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,  69, 125])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_inp_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the output (of each sequence from padded input sequence ) - ie the last word \n",
    "\n",
    "# final INPUT SEQUENCES \n",
    "X = pad_inp_seq[ : , :-1 ] # slect all rows(: ) and all columns ( except last one )-> (:-1 )\n",
    "\n",
    "#final OUTPUT sequenc\n",
    "y = pad_inp_seq[: , -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now , for Next-Word-Prediction Task using LSTM , we are actually doing \n",
    "# - MULTICLASS CLASSIFICATIOn since , we have to predict among multiple (discrete) classes  , a articular class .\n",
    "\n",
    "# USe oNe-HOT encoding on y(o/p) for better prediction \n",
    "\n",
    "from tensorflow.keras.utils import to_categorical \n",
    "\n",
    "#  no of unique words = 82 , so y_encoded.shaoe [82 x no of sentences ]\n",
    "y_encoded = to_categorical(y , num_classes = len(tokenize.word_index ) + 1 ) # +1 since OHE starts from 0 but tokenize.word_index starts from 1 # to cover last word also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886, 44)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM architecture -  (Embedding layer -> LSTM cell -> Desne Layer )\n",
    "from keras.layers import Embedding , LSTM , Dense \n",
    "from keras.models import Sequential \n",
    "\n",
    "# Embedding layer alwas at top of RNN or LSTM or GRU .\n",
    "\n",
    "uniq = len(tokenize.word_index) + 1 \n",
    "words_in_sen = X.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Embedding( unique words , output dim , no of words in 1 sentence)\n",
    "# EMbeddings can take vectorized words ALSo as input - to convert sparse vector Seq. to Dense vector seq ( for word representation)\n",
    "model.add( Embedding(input_dim =uniq , output_dim = 100  , input_length = words_in_sen ))  # 12 sicnce no of words in input sentence 12 \n",
    "\n",
    "model.add(LSTM(150 , return_sequences = True)   )  # No of nodes in each NN layer of each gate (forget , input , output )\n",
    "\n",
    "model.add(LSTM(60)) , # stacked LSTM *deep LSTM \n",
    "\n",
    "model.add( Dense(uniq, activation = 'softmax')) # multi-class classification \n",
    "\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 44, 100)           42900     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 44, 150)           150600    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 60)                50640     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 429)               26169     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 270309 (1.03 MB)\n",
      "Trainable params: 270309 (1.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nSHAPE OF each Layer of LSTM Architecture \\n\\nEmbedding - No of unique words (13) , No of Nimbers in output vector = 100 , So weights = (13 x 100 )\\nfor each unique word , we get a vector of dimesnion 100 x 1\\n\\nso for each sentence [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1] , we have (100 x 1 ) sized vector repres. for EACH word of this sentene-> (13  x 100 )\\n\\nN ow , send this [13 x 100 ] dimesnion vector to LSTM where (timesteps = no of words in 1 sentence  ) and (100 features for each word )\\n\\nand at each timestep ,w e pass SEQUENTIALLY [100] dim vvector of word 13 times to LSTM \\nand each time a hidden state (short temr meory) and LTM ( Cell state) will be of dim (150) since nodes =150\\nand next word prediction should lie among all unique words \\n\\npad_inp_seq[0].shape -> show no of  words in 1 sentence \\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "''' \n",
    "SHAPE OF each Layer of LSTM Architecture \n",
    "\n",
    "Embedding - No of unique words (13) , No of Nimbers in output vector = 100 , So weights = (13 x 100 )\n",
    "for each unique word , we get a vector of dimesnion 100 x 1\n",
    "\n",
    "so for each sentence [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1] , we have (100 x 1 ) sized vector repres. for EACH word of this sentene-> (13  x 100 )\n",
    "\n",
    "N ow , send this [13 x 100 ] dimesnion vector to LSTM where (timesteps = no of words in 1 sentence  ) and (100 features for each word )\n",
    "\n",
    "and at each timestep ,w e pass SEQUENTIALLY [100] dim vvector of word 13 times to LSTM \n",
    "and each time a hidden state (short temr meory) and LTM ( Cell state) will be of dim (150) since nodes =150\n",
    "and next word prediction should lie among all unique words \n",
    "\n",
    "pad_inp_seq[0].shape -> show no of  words in 1 sentence \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train , X_test , y_train , y_test = train_test_split(X , y_encoded , test_size= 0.2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "23/23 [==============================] - 7s 102ms/step - loss: 5.9702 - accuracy: 0.0339 - val_loss: 5.8607 - val_accuracy: 0.0337\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 2s 71ms/step - loss: 5.5679 - accuracy: 0.0410 - val_loss: 6.0056 - val_accuracy: 0.0337\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 2s 73ms/step - loss: 5.3904 - accuracy: 0.0410 - val_loss: 6.2026 - val_accuracy: 0.0337\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 2s 69ms/step - loss: 5.3490 - accuracy: 0.0410 - val_loss: 6.3283 - val_accuracy: 0.0506\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 2s 68ms/step - loss: 5.3347 - accuracy: 0.0424 - val_loss: 6.4229 - val_accuracy: 0.0506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a36b6369a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train , y_train , epochs = 5 , validation_data=[X_test , y_test] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# now , predict next word in a sentence \n",
    "\n",
    "\n",
    "# preprocess the text as INPUT for LSTM  ( intwger encoded vector of nums vector)\n",
    "# for tokenizeing and padding , pass INPUT sentence ALWAYS inside LIST\n",
    "\n",
    "\n",
    "\n",
    "# prediction for next 'N' words\n",
    "\n",
    "def predict_N_words (text , N ) :\n",
    "\n",
    "    \n",
    "    for i in range(N) :\n",
    "\n",
    "            # tokenize \n",
    "        seq = tokenize.texts_to_sequences([text])[0] # since itwas coming as list of list \n",
    "    \n",
    "    # pad the sequence \n",
    "        pad_seq = pad_sequences([seq] , maxlen = max_len , padding = 'pre')\n",
    "        seq = pad_seq[: , 1 : ]\n",
    "        \n",
    "        pos = np.argmax(model.predict(seq) ) # shows proabbility of each word as next wor d\n",
    "        # so select highest prob ability \n",
    "    \n",
    "        for word , index in tokenize.word_index.items() :\n",
    "            if index==pos :\n",
    "                print(word)\n",
    "                text  = text + \" \" + word\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 882ms/step\n",
      "to\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "to\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"India has a\"\n",
    "\n",
    "predict_N_words(text , 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated Recurrence Unit Architecture \n",
    "# Reset Gate -> ( rt x ht-1) what info to reatin and forget from prev hidden state . and also calculates candidate hiddens tate info \n",
    "#based on new input seq. \n",
    "\n",
    "# Update gate - find balance b/w  candidate cell state and new input data to calculate final hidden state info\n",
    "\n",
    "from keras.layers import GRU\n",
    "\n",
    "model_gru = Sequential()\n",
    "\n",
    "#Embedding( unique words , output dim , no of words in 1 sentence)\n",
    "# EMbeddings can take vectorized words ALSo as input - to convert sparse vector Seq. to Dense vector seq ( for word representation)\n",
    "model_gru.add( Embedding(input_dim =uniq , output_dim = 100  , input_length = words_in_sen ))  # 12 sicnce no of words in input sentence 12 \n",
    "\n",
    "model_gru.add(GRU(100)   )  # No of nodes in each NN layer of each gate (forget , input , output )\n",
    "\n",
    "\n",
    "model_gru.add( Dense(uniq, activation = 'softmax')) # multi-class classification  # stacked GRU\n",
    "\n",
    "\n",
    "model_gru.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "23/23 [==============================] - 3s 46ms/step - loss: 6.0501 - accuracy: 0.0254 - val_loss: 6.0315 - val_accuracy: 0.0337\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 1s 27ms/step - loss: 5.6877 - accuracy: 0.0367 - val_loss: 6.2688 - val_accuracy: 0.0337\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 5.3965 - accuracy: 0.0523 - val_loss: 6.2057 - val_accuracy: 0.0506\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 1s 28ms/step - loss: 5.3232 - accuracy: 0.0438 - val_loss: 6.5107 - val_accuracy: 0.0562\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 1s 29ms/step - loss: 5.2719 - accuracy: 0.0452 - val_loss: 6.5243 - val_accuracy: 0.0562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a374e5ec70>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gru.fit(X_train , y_train , epochs = 5 , validation_data=[X_test , y_test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRUs are much faster (due to less trainable param to train upon) \n",
    "# better accuracy on smaller datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# now , predict next word in a sentence \n",
    "\n",
    "\n",
    "# preprocess the text as INPUT for LSTM  ( intwger encoded vector of nums vector)\n",
    "# for tokenizeing and padding , pass INPUT sentence ALWAYS inside LIST\n",
    "\n",
    "\n",
    "\n",
    "# prediction for next 'N' words\n",
    "\n",
    "def predict_N_words_gru (text , N ) :\n",
    "\n",
    "    \n",
    "    for i in range(N) :\n",
    "\n",
    "            # tokenize \n",
    "        seq = tokenize.texts_to_sequences([text])[0] # since itwas coming as list of list \n",
    "    \n",
    "    # pad the sequence \n",
    "        pad_seq = pad_sequences([seq] , maxlen = max_len , padding = 'pre')\n",
    "        seq = pad_seq[: , 1 : ]\n",
    "        \n",
    "        pos = np.argmax(model_gru.predict(seq) ) # shows proabbility of each word as next wor d\n",
    "        # so select highest prob ability \n",
    "    \n",
    "        for word , index in tokenize.word_index.items() :\n",
    "            if index==pos :\n",
    "                print(word)\n",
    "                text  = text + \" \" + word\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "to\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "text = \"delivering with equity is the \"\n",
    "predict_N_words(text , 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU -> keras.layers.GRU( units of neurons in each NN , activation )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Deep RNN (Stacked RNN hidden layers ) where intial layers - undersatnd simple word s\n",
    "# midddle layers understand phrases \n",
    "# last layers understand sentences and review s for final output . \n",
    "\n",
    "from keras.layers import SimpleRNN\n",
    "\n",
    "deep_rnn = Sequential()\n",
    "\n",
    "#Embedding( unique words , output dim , no of words in 1 sentence)\n",
    "# EMbeddings can take vectorized words ALSo as input - to convert sparse vector Seq. to Dense vector seq ( for word representation)\n",
    "deep_rnn.add( Embedding(input_dim =uniq , output_dim = 100  , input_length = words_in_sen ))  # 12 sicnce no of words in input sentence 12 \n",
    " \n",
    "deep_rnn.add( SimpleRNN(5 , return_sequences = True)   )   # first hidden layer of 5  rnn cells \n",
    "\n",
    "# stacked RNN cells \n",
    "# use return sequences = True for every layer except last RNN hidden layer \n",
    "\n",
    "deep_rnn.add( SimpleRNN(10)  )\n",
    "\n",
    "deep_rnn.add(Dense(uniq , activation = 'softmax'))\n",
    "\n",
    "\n",
    "deep_rnn.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 44, 100)           42900     \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, 44, 5)             530       \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 10)                160       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 429)               4719      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 48309 (188.71 KB)\n",
      "Trainable params: 48309 (188.71 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "deep_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 2s 27ms/step - loss: 6.0422 - accuracy: 0.0198 - val_loss: 6.0416 - val_accuracy: 0.0112\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 5.9473 - accuracy: 0.0367 - val_loss: 6.0258 - val_accuracy: 0.0169\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 5.8529 - accuracy: 0.0367 - val_loss: 6.0151 - val_accuracy: 0.0169\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 5.7583 - accuracy: 0.0367 - val_loss: 6.0122 - val_accuracy: 0.0112\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 0s 12ms/step - loss: 5.6732 - accuracy: 0.0424 - val_loss: 6.0210 - val_accuracy: 0.0112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a36fbadf70>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deep_rnn.fit(X_train , y_train , epochs  =5 , validation_data= [X_test , y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# now , predict next word in a sentence \n",
    "\n",
    "\n",
    "# preprocess the text as INPUT for LSTM  ( intwger encoded vector of nums vector)\n",
    "# for tokenizeing and padding , pass INPUT sentence ALWAYS inside LIST\n",
    "\n",
    "\n",
    "\n",
    "# prediction for next 'N' words\n",
    "\n",
    "def predict_N_words_deep_rnn (text , N ) :\n",
    "\n",
    "    \n",
    "    for i in range(N) :\n",
    "\n",
    "            # tokenize \n",
    "        seq = tokenize.texts_to_sequences([text])[0] # since itwas coming as list of list \n",
    "    \n",
    "    # pad the sequence \n",
    "        pad_seq = pad_sequences([seq] , maxlen = max_len , padding = 'pre')\n",
    "        seq = pad_seq[: , 1 : ]\n",
    "        \n",
    "        pos = np.argmax(deep_rnn.predict(seq) ) # shows proabbility of each word as next wor d\n",
    "        # so select highest prob ability \n",
    "    \n",
    "        for word , index in tokenize.word_index.items() :\n",
    "            if index==pos :\n",
    "                print(word)\n",
    "                text  = text + \" \" + word\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 223ms/step\n",
      "a\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "a\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "a\n"
     ]
    }
   ],
   "source": [
    "text = \"delivering with equity is  \"\n",
    "predict_N_words_deep_rnn(text , 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bidirectional RNN - forward rnn + backward rnn \n",
    "# forward rnn processes info. from front side \n",
    "# backward rnn processes info from back side \n",
    "# and both outputs are concatenated together and final o/p is generated \n",
    "\n",
    "# when future outputs affect past inputs \n",
    "\n",
    "# Use Deep RNN (Stacked RNN hidden layers ) where intial layers - undersatnd simple word s\n",
    "# midddle layers understand phrases \n",
    "# last layers understand sentences and review s for final output . \n",
    "\n",
    "from keras.layers import Bidirectional\n",
    "\n",
    "bi_rnn = Sequential()\n",
    "\n",
    "#Embedding( unique words , output dim , no of words in 1 sentence)\n",
    "# EMbeddings can take vectorized words ALSo as input - to convert sparse vector Seq. to Dense vector seq ( for word representation)\n",
    "bi_rnn.add( Embedding(input_dim =uniq , output_dim = 100  , input_length = words_in_sen ))  # 12 sicnce no of words in input sentence 12 \n",
    " \n",
    "bi_rnn.add( Bidirectional( SimpleRNN(5)) ) # in keras , use bidirectional(LSTM/GRU/RNN cell) to form bidir rnn layer \n",
    "# weights double as info processe from both sides of sentence / seq data \n",
    "\n",
    "\n",
    "bi_rnn.add(Dense(uniq , activation = 'softmax'))\n",
    "\n",
    "\n",
    "bi_rnn.compile(loss = 'categorical_crossentropy' , optimizer = 'adam' , metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "23/23 [==============================] - 2s 27ms/step - loss: 6.0581 - accuracy: 0.0014 - val_loss: 6.0490 - val_accuracy: 0.0112\n",
      "Epoch 2/5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 6.0130 - accuracy: 0.0297 - val_loss: 6.0375 - val_accuracy: 0.0393\n",
      "Epoch 3/5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 5.9552 - accuracy: 0.0480 - val_loss: 6.0351 - val_accuracy: 0.0787\n",
      "Epoch 4/5\n",
      "23/23 [==============================] - 0s 13ms/step - loss: 5.8741 - accuracy: 0.0579 - val_loss: 6.0294 - val_accuracy: 0.0618\n",
      "Epoch 5/5\n",
      "23/23 [==============================] - 0s 14ms/step - loss: 5.7767 - accuracy: 0.0480 - val_loss: 6.0298 - val_accuracy: 0.0562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1a37b00fd90>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn.fit( X_train , y_train , epochs  = 5 , validation_data= [X_test , y_test] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# now , predict next word in a sentence \n",
    "\n",
    "\n",
    "def predict_N_words_deep_rnn (text , N ) :\n",
    "\n",
    "    \n",
    "    for i in range(N) :\n",
    "\n",
    "            # tokenize \n",
    "        seq = tokenize.texts_to_sequences([text])[0] # since itwas coming as list of list \n",
    "    \n",
    "    # pad the sequence \n",
    "        pad_seq = pad_sequences([seq] , maxlen = max_len , padding = 'pre')\n",
    "        seq = pad_seq[: , 1 : ]\n",
    "        \n",
    "        pos = np.argmax(bi_rnn.predict(seq) ) # shows proabbility of each word as next wor d\n",
    "        # so select highest prob ability \n",
    "    \n",
    "        for word , index in tokenize.word_index.items() :\n",
    "            if index==pos :\n",
    "                print(word)\n",
    "                text  = text + \" \" + word\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 239ms/step\n",
      "to\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "to\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "to\n"
     ]
    }
   ],
   "source": [
    "text = \"adjust to the fact\"\n",
    "predict_N_words_deep_rnn(text , 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickling the model \n",
    "\n",
    "import pickle \n",
    "\n",
    "pickle.dump(bi_rnn , open( 'model.pkl' , 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
